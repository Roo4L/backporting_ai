<!DOCTYPE html><html lang="en" class="h-full"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="Digest No. 2 – 2025-12-01"><link rel="icon" href="/pr-preview/pr-4/favicon.svg"><title>Backporting.ai Digest – November 2025</title><!-- Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=G-4CFT2R9P8Y"></script><script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-4CFT2R9P8Y');
    </script><link rel="stylesheet" href="/pr-preview/pr-4/_astro/about.BWOesywI.css"></head> <body class="h-full bg-white text-slate-900"> <header class="border-b bg-white/80 backdrop-blur"> <div class="mx-auto max-w-6xl px-4 py-4 flex items-center justify-between"> <a href="/pr-preview/pr-4/" class="text-xl font-semibold bg-gradient-to-r from-sky-700 to-indigo-700 bg-clip-text text-transparent">Backporting.ai</a> <nav class="space-x-6 text-sm"> <a class="text-slate-700 hover:text-sky-700" href="/pr-preview/pr-4/articles">Articles</a> <a class="text-slate-700 hover:text-sky-700" href="/pr-preview/pr-4/digests">Digests</a> <a class="text-slate-700 hover:text-sky-700" href="/pr-preview/pr-4/about">About</a> </nav> </div> </header> <main class="mx-auto max-w-6xl px-4 py-12">  <article class="mx-auto max-w-3xl"> <h1 class="text-3xl sm:text-4xl font-semibold">Backporting.ai Digest – November 2025</h1> <p class="text-sm text-slate-500">
No. 2 · <time datetime="2025-12-01">2025-12-01</time> · By <a href="https://deistvitelnobesconechna.blog/" target="_blank" rel="noopener noreferrer" class="text-slate-700 hover:text-sky-700 hover:underline inline-flex items-center gap-1"> Nikita Ivanov <svg class="w-3 h-3 inline-block" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"> <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14"></path> </svg> </a> </p> <p class="mt-6 text-lg leading-relaxed text-slate-700">This month&#39;s four papers all focus on automated program repair: history-aware patching, Ruby support, vulnerability benchmarks, and kernel driver migration.</p> <div class="mt-8 text-slate-800"><h2 class="mt-12 mb-4 text-2xl font-semibold text-slate-900">Mining Git History for Better Patches</h2>
<p class="mt-5 text-base leading-loose"><a href="https://arxiv.org/abs/2511.01047" target="_blank" rel="noopener noreferrer" class="underline decoration-slate-300 hover:decoration-slate-500">HAFixAgent</a> takes a page from how human developers actually debug: before writing a fix, look at who changed what and why. The system hooks into git-blame to surface version control history—previous commits, related changes, the evolution of the buggy code—and feeds this context into an agentic repair loop.</p>
<p class="mt-5 text-base leading-loose">Multi-hunk bugs are where most automated repair tools stumble. When a fix requires coordinated edits across multiple locations, single-shot generation rarely gets it right. HAFixAgent addresses this by treating history as a first-class signal: if a function was refactored three commits ago and introduced a subtle regression, that commit diff becomes part of the repair context. The agent can trace the bug to its origin and reason about what the original author intended.</p>
<p class="mt-5 text-base leading-loose">The evaluation uses Defects4J, the standard benchmark of real-world Java bugs collected from open-source projects. Across 854 bugs, 71.1% had accessible blame information, and 70.7% mapped to a single unique blame commit—repository history is both widely available and highly concentrated. HAFixAgent repaired 523 out of 829 common bugs. For comparison, the best prior LLM-based repair agent fixed only 164 of those same bugs—HAFixAgent more than triples that baseline. For the harder multi-hunk cases (371 bugs requiring coordinated edits across multiple locations), HAFixAgent fixed 175 compared to 133 for the previous best multi-hunk repair tool—a 29.9% improvement. The efficiency held too: incorporating historical context didn&#39;t significantly increase agent steps or token costs, and for the most complex multi-file-multi-hunk bugs, median costs were actually lower.</p>
<h2 class="mt-12 mb-4 text-2xl font-semibold text-slate-900">Ruby Gets Its Own Repair System</h2>
<p class="mt-5 text-base leading-loose">Automated program repair has focused heavily on Java, Python, and C. Ruby&#39;s dynamic typing and metaprogramming make it a harder target. <a href="https://arxiv.org/abs/2511.03925" target="_blank" rel="noopener noreferrer" class="underline decoration-slate-300 hover:decoration-slate-500">RAMP</a> brings collaborative agents to Ruby with a test-driven feedback architecture.</p>
<p class="mt-5 text-base leading-loose">The system splits responsibilities across specialized agents. One handles fault localization—identifying which methods and lines are likely buggy based on failing test cases. Another generates candidate patches. A third validates patches against the test suite and filters plausible fixes. The agents communicate through shared context, iteratively refining patches based on test feedback.</p>
<p class="mt-5 text-base leading-loose">Evaluated on a multilingual code benchmark, RAMP fixes 67% of Ruby bugs on the first attempt—the first comprehensive evaluation of LLM-based repair on this language. The system converges quickly, typically within five iterations. Ablation studies confirm the key drivers: test generation and self-reflection are essential components. Without them, performance drops significantly. RAMP handles the full spectrum of failure modes—wrong answers, compilation errors, and runtime crashes—providing new insights into what makes multi-agent repair effective for dynamically-typed languages underserved by existing repair research.</p>
<h2 class="mt-12 mb-4 text-2xl font-semibold text-slate-900">Benchmarking Vulnerability Patching Across Languages</h2>
<p class="mt-5 text-base leading-loose">How well do LLMs actually patch real vulnerabilities? <a href="https://arxiv.org/abs/2511.11019" target="_blank" rel="noopener noreferrer" class="underline decoration-slate-300 hover:decoration-slate-500">PATCHEVAL</a> provides a multilingual benchmark grounded in actual security vulnerabilities (the kind that get CVE identifiers and public disclosures), covering Go, JavaScript, and Python—languages underrepresented in existing vulnerability repair datasets. Most benchmarks focus on C/C++ or Java; PATCHEVAL addresses this gap with real-world vulnerabilities from modern web and systems programming ecosystems.</p>
<p class="mt-5 text-base leading-loose">The benchmark categorizes each vulnerability by weakness type—input validation flaws, injection bugs, authentication bypasses, and so on. This enables fine-grained analysis of which weakness types LLMs handle well and where they fail. Input validation bugs might be straightforward; injection vulnerabilities requiring semantic understanding of data flow pose different challenges. The categorization lets researchers identify systematic blind spots rather than treating all vulnerabilities as equivalent.</p>
<p class="mt-5 text-base leading-loose">PATCHEVAL includes sandbox-based patch validation rather than just checking syntactic correctness. Generated patches run against test suites and exploit-triggering inputs to verify they actually fix the vulnerability without breaking functionality. This catches the common failure mode where a patch &quot;fixes&quot; the bug by removing the vulnerable functionality entirely—a technically correct but practically useless repair.</p>
<p class="mt-5 text-base leading-loose">The multilingual scope matters because vulnerabilities manifest differently across language ecosystems. A SQL injection in a Python web app has different fix patterns than an equivalent bug in a Go service. Cross-language benchmarks reveal whether repair capabilities transfer or whether models need language-specific fine-tuning.</p>
<h2 class="mt-12 mb-4 text-2xl font-semibold text-slate-900">Keeping Drivers Alive Across Kernel Versions</h2>
<p class="mt-5 text-base leading-loose">Out-of-tree drivers face a maintenance treadmill: every kernel release potentially breaks API compatibility. A driver working on 5.15 might fail to compile on 6.1 because subsystem interfaces changed. <a href="https://arxiv.org/abs/2511.18924" target="_blank" rel="noopener noreferrer" class="underline decoration-slate-300 hover:decoration-slate-500">AUTODRIVER</a> automates this migration using multi-agent LLMs.</p>
<p class="mt-5 text-base leading-loose">The system introduces DRIVEBENCH, a benchmark of real driver update scenarios extracted from Linux kernel evolution. Each case pairs a driver with its source kernel version and a target version where API changes broke compatibility. The benchmark captures the diversity of kernel API churn—renamed functions, restructured data types, deprecated interfaces, changed calling conventions. Unlike synthetic benchmarks, DRIVEBENCH draws from actual historical migrations that kernel maintainers performed manually, providing ground truth for evaluating automated approaches.</p>
<p class="mt-5 text-base leading-loose">AUTODRIVER&#39;s multi-agent architecture separates concerns: one agent analyzes the API changes between kernel versions, another localizes which driver code needs updating, and a third generates the actual patches. This decomposition mirrors how kernel developers approach the problem—first understand what changed in the subsystem, then systematically update each affected callsite. The multi-agent design prevents any single model from being overwhelmed by the complexity of kernel code while allowing specialization.</p>
<p class="mt-5 text-base leading-loose">The focus on drivers is strategic. Drivers constitute a huge fraction of kernel code—over 60% by some estimates—much of it maintained by small teams or individual developers who struggle to keep up with upstream churn. Automated migration could keep hardware support alive across kernel versions without requiring constant manual porting effort, particularly valuable for industrial and embedded systems locked to specific hardware.</p></div> <section class="mt-12"> <h2 class="text-2xl font-semibold">References</h2> <ul class="mt-4 divide-y divide-slate-200"> <li class="py-4"> <div class="flex flex-col gap-2"> <a href="https://arxiv.org/abs/2511.01047" target="_blank" rel="noopener noreferrer" class="font-medium text-slate-900 hover:text-sky-700 hover:underline"> HAFixAgent: History-Aware Automated Program Repair Agent </a> </div> <div class="text-sm text-slate-600 mt-2 leading-relaxed">Uses git-blame and version control history to improve multi-hunk patch generation; fixes 523/829 Java bugs, tripling prior best results.</div> <div class="mt-2 text-xs text-slate-500"> <span class="mr-2">#research</span><span class="mr-2">#program-repair</span><span class="mr-2">#LLM</span><span class="mr-2">#agent</span><span class="mr-2">#history-aware</span><span class="mr-2">#git-blame</span><span class="mr-2">#multi-hunk</span> </div> </li><li class="py-4"> <div class="flex flex-col gap-2"> <a href="https://arxiv.org/abs/2511.03925" target="_blank" rel="noopener noreferrer" class="font-medium text-slate-900 hover:text-sky-700 hover:underline"> Collaborative Agents for Automated Program Repair in Ruby </a> </div> <div class="text-sm text-slate-600 mt-2 leading-relaxed">Multi-agent system with test-driven feedback for Ruby program repair; achieves 67% fix rate on first attempt.</div> <div class="mt-2 text-xs text-slate-500"> <span class="mr-2">#research</span><span class="mr-2">#program-repair</span><span class="mr-2">#LLM</span><span class="mr-2">#multi-agent</span><span class="mr-2">#Ruby</span><span class="mr-2">#test-driven</span> </div> </li><li class="py-4"> <div class="flex flex-col gap-2"> <a href="https://arxiv.org/abs/2511.11019" target="_blank" rel="noopener noreferrer" class="font-medium text-slate-900 hover:text-sky-700 hover:underline"> PATCHEVAL: A New Benchmark for Evaluating LLMs on Patching Real-World Vulnerabilities </a> </div> <div class="text-sm text-slate-600 mt-2 leading-relaxed">Multilingual benchmark of real security vulnerabilities across Go, JavaScript, and Python with sandbox-based patch validation.</div> <div class="mt-2 text-xs text-slate-500"> <span class="mr-2">#research</span><span class="mr-2">#security</span><span class="mr-2">#benchmark</span><span class="mr-2">#LLM</span><span class="mr-2">#vulnerability-repair</span><span class="mr-2">#multilingual</span><span class="mr-2">#patch-validation</span> </div> </li><li class="py-4"> <div class="flex flex-col gap-2"> <a href="https://arxiv.org/abs/2511.18924" target="_blank" rel="noopener noreferrer" class="font-medium text-slate-900 hover:text-sky-700 hover:underline"> LLM-Driven Kernel Evolution: Automating Driver Updates in Linux </a> </div> <div class="text-sm text-slate-600 mt-2 leading-relaxed">Multi-agent system for automatically migrating Linux kernel drivers across API changes; introduces a benchmark of real driver update scenarios.</div> <div class="mt-2 text-xs text-slate-500"> <span class="mr-2">#research</span><span class="mr-2">#Linux-kernel</span><span class="mr-2">#driver-maintenance</span><span class="mr-2">#LLM</span><span class="mr-2">#multi-agent</span><span class="mr-2">#automated-patching</span><span class="mr-2">#API-changes</span> </div> </li> </ul> </section> </article>  </main> <footer class="border-t border-slate-200 bg-white"> <div class="mx-auto max-w-6xl px-4 py-6 text-sm text-slate-600 flex flex-col sm:flex-row items-center justify-between gap-3"> <p>© 2026 Backporting.ai · Curated resources on patch backporting automation.</p> <div class="flex items-center gap-4"> <a href="mailto:hello@backporting.ai" class="hover:text-slate-800">hello@backporting.ai</a> <a href="https://github.com/Roo4L/backporting_ai" class="hover:text-slate-800" rel="noopener noreferrer">GitHub</a> </div> </div> </footer>  </body></html>